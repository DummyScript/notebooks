{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# wikipedia stuff\n",
    "import os, sys, datetime, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import ag.logging as log\n",
    "buf=\"$\" * 40\n",
    "log.set(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Number of Staring Characters: 1288556\n",
      "# Number of Working Characters: 257711\n"
     ]
    }
   ],
   "source": [
    "# open dataset\n",
    "class dataset(): pass\n",
    "dataset.text = open('datasets/wiki/wiki.test.raw', encoding='utf-8', errors=\"surrogateescape\").read()\n",
    "dataset.len = len(dataset.text)\n",
    "print(\"# Number of Staring Characters: {}\".format(dataset.len))\n",
    "# oh NOOOO!!!! we have to trunkate or dataset in a weird place... this could hoze our results!\n",
    "dataset.text = dataset.text[:int(dataset.len/5)] # gotta cut it by 80%\n",
    "dataset.len = len(dataset.text)\n",
    "print(\"# Number of Working Characters: {}\".format(dataset.len))\n",
    "dataset.sample = dataset.text[:100]\n",
    "dataset.chars = sorted(list(set(dataset.text)))\n",
    "dataset.len_chars = len(dataset.chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Number of Character: 257711\n",
      "# Number of Unique Characters: 149\n",
      "# Sample:  \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had \n",
      "\n",
      "# All Unique Characters:\n",
      " ['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', 'à', 'á', 'ã', 'æ', 'è', 'é', 'í', 'ñ', 'ó', 'ü', 'ě', 'ī', 'Ō', 'ō', 'ū', 'ǐ', 'ǜ', 'ə', 'έ', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ό', '–', '—', '’', '−', '♯', '伊', '傳', '八', '勢', '史', '型', '士', '大', '律', '成', '戦', '春', '望', '杜', '東', '甫', '甲', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '鉄', '集']\n"
     ]
    }
   ],
   "source": [
    "print(\"# Number of Character: {}\".format(dataset.len))\n",
    "print(\"# Number of Unique Characters: {}\".format(dataset.len_chars))\n",
    "print(\"# Sample: {}\\n\".format(dataset.sample))\n",
    "print(\"# All Unique Characters:\\n {}\".format(dataset.chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(dataset.chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(dataset.chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# probibilty of each next character\n",
    "def sample(pred):\n",
    "    r = random.uniform(0,1)\n",
    "    s= 0\n",
    "    l = len(pred)\n",
    "    char_id =  l-1 \n",
    "    for i in range(l):\n",
    "        s += pred[i]\n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    \n",
    "    one_hot_char = np.zeros(shape[char_size])\n",
    "    one_hot_char[char_id] = 1.0\n",
    "    return one_hot_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# batch and options\n",
    "class options(): pass\n",
    "options.batch_section = 50\n",
    "options.skip = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Section: 128831\n"
     ]
    }
   ],
   "source": [
    "# do some work... create the sections list\n",
    "dataset.sections = []\n",
    "dataset.next_chars = []\n",
    "for i in range(0, dataset.len - options.batch_section, options.skip):\n",
    "    dataset.sections.append(dataset.text[i: i + options.batch_section])\n",
    "    dataset.next_chars.append(dataset.text[i + options.batch_section])\n",
    "dataset.len_sections = len(dataset.sections)\n",
    "print(\"Length of Section: {}\".format(dataset.len_sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# if you want to explore your memory size... go for /4\n",
    "# x = np.zeros((int(644253/5), int(50), 159))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create input data sctructure placeholders\n",
    "tensor_inputs = np.zeros((dataset.len_sections, options.batch_section, dataset.len_chars))\n",
    "tensor_labels = np.zeros((dataset.len_sections, dataset.len_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of labels:\t128831\n",
      "Labels: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# for each_position and each batch of 50, one at a time, in all batches with index.\n",
    "for i, section in enumerate(dataset.sections):\n",
    "    # for this poistion and this character in this section with its index\n",
    "    for j, char in enumerate(section):\n",
    "        # place a datapoint at for T at poistion 4000 h is the next char\n",
    "        tensor_inputs[i,j,char2id[char]] = 1\n",
    "    tensor_labels[i, char2id[dataset.next_chars[i]]] = 1\n",
    "print(\"Len of labels:\\t{}\\nLabels: {}\".format(len(tensor_labels),tensor_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### BOOMSKI here we go... data wash complete.\n",
    "options.batch_size = 512\n",
    "options.iters = 1e5\n",
    "options.log_every = 100\n",
    "options.save_every = 6000\n",
    "options.hidden_nodes = 1024\n",
    "options.max_text = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save path is: /home/eric/datasets/wiki/log/perm/wikigen\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"The world is filled with music.\"\n",
    "if True:\n",
    "    save_path = \"/home/eric/datasets/wiki/log/perm/wikigen\"\n",
    "    if os.path.isdir(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        \"had to make the dir.\"\n",
    "        tf.gfile.MakeDirs(save_path)\n",
    "else:    \n",
    "    save_path = \"/home/eric/datasets/wiki/log\"\n",
    "    if os.path.isdir(save_path):\n",
    "        print(\"check\")\n",
    "        all_logs = os.listdir(save_path)\n",
    "        num_logs = len(all_logs)\n",
    "        new_path = os.path.join(save_path,\"wikigen_{}\".format(num_logs))\n",
    "        tf.gfile.MakeDirs(new_path)\n",
    "        if os.path.isdir(new_path):\n",
    "            save_path = new_path\n",
    "        else:\n",
    "            print(\"error creating folder\")\n",
    "\n",
    "    else:\n",
    "        print(\"errors finding path\")\n",
    "print(\"Save path is: {}\".format(save_path))\n",
    "save_filename = \"ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 128831\n",
      "steps per epoch: 251\n"
     ]
    }
   ],
   "source": [
    "print(\"training data size: {}\".format(len(tensor_inputs)))\n",
    "print(\"steps per epoch: {}\".format(int(len(tensor_inputs)/options.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inputs\"):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # global_step = tf.Variable('global_step',trainable=False)\n",
    "    learn_rate = tf.train.exponential_decay(\n",
    "            0.1, global_step,\n",
    "            1e-7, 0.87, staircase=True,\n",
    "            name=\"Learn_decay\")\n",
    "\n",
    "    tf.add_to_collection(\"learn_rate\", learn_rate)\n",
    "    tf.summary.scalar(\"learn_rate\", learn_rate)\n",
    "    # input placeholders still inside of inputs scope\n",
    "    input_holder = tf.placeholder(tf.float32, \n",
    "                                  [options.batch_size,\n",
    "                                   options.batch_section,\n",
    "                                   dataset.len_chars],\n",
    "                                  name=\"Input_Tensor\")\n",
    "\n",
    "    input_label = tf.placeholder(tf.float32, \n",
    "                                 [options.batch_size,\n",
    "                                  dataset.len_chars],\n",
    "                                 name=\"Input_label\")\n",
    "tf.add_to_collection(\"input_tensor\", input_holder)\n",
    "tf.add_to_collection(\"input_label\", input_label)\n",
    "# input game, out game, forget gate, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"gates\"):\n",
    "    #'''input gate'''\n",
    "    # weights for input\n",
    "    w_ii = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"in_weights\")\n",
    "    # weights for previous output\n",
    "    w_io = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"previous_weights\")\n",
    "    # bias\n",
    "    b_i = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"input_bias\")\n",
    "    # for re optimizing\n",
    "    tf.add_to_collection(\"in_weights\", w_ii)\n",
    "    tf.add_to_collection(\"previous_weights\", w_io)\n",
    "    tf.add_to_collection(\"input_bias\", b_i)\n",
    "\n",
    "    #'''forget gate'''\n",
    "    # weights for input\n",
    "    w_fi = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"forget_weights\")\n",
    "    # weights for previous output\n",
    "    w_fo = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"forget_previous_weights\")\n",
    "    # bias\n",
    "    b_f = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"forget_bias\")\n",
    "    # for re optimizing\n",
    "    tf.add_to_collection(\"forget_weights\", w_fi)\n",
    "    tf.add_to_collection(\"forget_previous_weights\", w_fo)\n",
    "    tf.add_to_collection(\"forget_bias\", b_f)\n",
    "\n",
    "    #'''output gate'''\n",
    "    # weights for input\n",
    "    w_oi = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"out_weights\")\n",
    "    # weights for previous output\n",
    "    w_oo = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"out_previous_weights\")\n",
    "    # bias\n",
    "    b_o = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"output_bias\")\n",
    "    # for re optimizing\n",
    "    tf.add_to_collection(\"output_weights\", w_oi)\n",
    "    tf.add_to_collection(\"out_previous_weights\", w_oo)\n",
    "    tf.add_to_collection(\"output_bias\", b_o)\n",
    "\n",
    "    #'''memory gate'''\n",
    "    # weights for input\n",
    "    w_ci = tf.Variable(tf.truncated_normal([dataset.len_chars,\n",
    "                                           options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"memory_weights\")\n",
    "    # weights for previous output\n",
    "    w_co = tf.Variable(tf.truncated_normal([options.hidden_nodes,\n",
    "                                            options.hidden_nodes],\n",
    "                                           -0.1,0.1),\n",
    "                       name=\"memory_previous_weights\")\n",
    "    # bias\n",
    "    b_c = tf.Variable(tf.zeros([1, options.hidden_nodes]),\n",
    "                      name=\"memory_bias\")\n",
    "    # for re optimizing\n",
    "    tf.add_to_collection(\"memory_weights\", w_ci)\n",
    "    tf.add_to_collection(\"memory_previous_weights\", w_co)\n",
    "    tf.add_to_collection(\"memory_bias\", b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "    #(input * forget weights) + (output * weights for previous output) + bias\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "    #(input * output weights) + (output * weights for previous output) + bias\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "    #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "    memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "\n",
    "    #...now! multiply forget gate * given state    +  input gate * hidden state\n",
    "    state = forget_gate * state + input_gate * memory_cell\n",
    "    #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "    #multiply by output\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### OHHHH WE ARE GETTING CLOSE! ###\n",
    "# \"\"\" Initialize the session \"\"\"\n",
    "# init_op = tf.global_variables_initializer\n",
    "# \n",
    "# \"\"\"create summary op\"\"\"\n",
    "# merged = tf.summary.merge_all\n",
    "# \n",
    "# \"\"\"create saver object\"\"\"\n",
    "# saver = tf.train.Saver(\n",
    "#             #var_list={\"{}\".format(v): v for v in [tf.model_variables()]},\n",
    "#             write_version=tf.train.SaverDef.V2,\n",
    "#             sharded=True,\n",
    "#             keep_checkpoint_every_n_hours=.001\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros:0\", shape=(512, 1024), dtype=float32) Tensor(\"zeros_1:0\", shape=(512, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output = tf.zeros([options.batch_size, options.hidden_nodes])\n",
    "state = tf.zeros([options.batch_size, options.hidden_nodes])\n",
    "print(output, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working first set\n",
      "working final set\n",
      "process complete\n"
     ]
    }
   ],
   "source": [
    "###... still chopping wood...\n",
    "outputs_all_i = []\n",
    "labels_all_i = []\n",
    "for i in range(options.batch_section):\n",
    "    #calculate state and output from LSTM\n",
    "    output, state = lstm_cell(input_holder[:, i, :], output, state)\n",
    "    #to start, \n",
    "    if i == 0:\n",
    "        print(\"working first set\")\n",
    "        #store initial output and labels\n",
    "        outputs_all_i = output\n",
    "        labels_all_i = input_holder[:, i+1, :]\n",
    "    #for each new set, concat outputs and labels\n",
    "    if i != options.batch_section - 1:\n",
    "        # print(\"working set {}\".format(i+1))\n",
    "        #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "        outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "        labels_all_i = tf.concat([labels_all_i, input_holder[:, i+1, :]], 0)\n",
    "    else:\n",
    "        #final store\n",
    "        print(\"working final set\")\n",
    "        outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "        labels_all_i = tf.concat([labels_all_i, input_label], 0)\n",
    "print(\"process complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Classificator\n",
    "with tf.variable_scope(\"superNet_layer\"):\n",
    "    sw = tf.Variable(tf.truncated_normal([options.hidden_nodes, dataset.len_chars], -0.1,0.1),\n",
    "                    name=\"superNet_weights\")\n",
    "    sb = tf.Variable(tf.zeros(dataset.len_chars),name=\"superNet_biases\")\n",
    "tf.summary.histogram('superNet_weights', sw); \n",
    "tf.summary.histogram('superNet_biases', sb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# with tf.device(\"gpu0\"):\n",
    "with tf.variable_scope(\"training_op\"):\n",
    "    # google like the word logits.. it represents the \"final Layer\"\n",
    "    final_layer = tf.matmul(outputs_all_i, sw) + sb\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_all_i, logits=final_layer))\n",
    "    tf.add_to_collection(\"softmax_activations\", cost);\n",
    "    tf.summary.scalar(\"softmax_activations\", cost);\n",
    "    tf.summary.histogram('softmax_activations', cost);\n",
    "    # should have them all here lined up... but im not there yet...\n",
    "    #train_op = tf.train.AdagradOptimizer(learn_rate).minimize(cost, global_step=global_step)\n",
    "    train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "    tf.add_to_collection(\"train_op\", train_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data = tf.placeholder(tf.float32, shape=[1, dataset.len_chars])\n",
    "test_output = tf.Variable(tf.zeros([1, options.hidden_nodes]))\n",
    "test_state = tf.Variable(tf.zeros([1, options.hidden_nodes]))\n",
    "\n",
    "#Reset at the beginning of each test\n",
    "reset_test_state = tf.group(test_output.assign(tf.zeros([1, options.hidden_nodes])), \n",
    "                            test_state.assign(tf.zeros([1, options.hidden_nodes])))\n",
    "\n",
    "#LSTM\n",
    "test_output, test_state = lstm_cell(test_data, test_output, test_state)\n",
    "with tf.variable_scope(\"test_op\"):\n",
    "    test_op = tf.nn.softmax(tf.matmul(test_output, sw) + sb)\n",
    "    tf.add_to_collection(\"test_softmax_activations\", test_op);\n",
    "    tf.summary.scalar(\"test_softmax_activations\", test_op);\n",
    "    tf.summary.histogram('test_softmax_activations', test_op);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### OHHHH WE ARE GETTING CLOSE! ###\n",
    "# always need them in a collection if we plan to retrieve them later.\n",
    "# with tf.variable_scope(\"init_stuffs\"):\n",
    "\"\"\" Initialize the session \"\"\"\n",
    "init_op = tf.global_variables_initializer\n",
    "\n",
    "\"\"\"create summary op\"\"\"\n",
    "merged = tf.summary.merge_all\n",
    "\n",
    "\"\"\"create saver object\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.Saver(\n",
    "#             #var_list={\"{}\".format(v): v for v in [tf.model_variables()]},\n",
    "#             write_version=tf.train.SaverDef.V2,\n",
    "#             sharded=True,\n",
    "#             keep_checkpoint_every_n_hours=.001\n",
    "#             )\n",
    "# tf.add_to_collection(\"init_op\", init_op);\n",
    "# tf.add_to_collection(\"summary\", merged);\n",
    "# tf.add_to_collection(\"save\", saver);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *I* Starting the wikigen Training!\n",
      " ~D~ session started\n",
      " ~D~ init started\n",
      " *I* Training for 1 iters\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Starting the wikigen Training!\")\n",
    "#with tf.Graph().as_default():\n",
    "try:\n",
    "    if sess:\n",
    "        log.debug(\"closing old sess\")\n",
    "        sess.close()\n",
    "except: pass\n",
    "sess = tf.InteractiveSession()\n",
    "log.debug(\"session started\")\n",
    "#init graph, load model\n",
    "sess.run(init_op())\n",
    "#train_writer = tf.summary.FileWriter(save_path, sess.graph)\n",
    "# test_writer = tf.summary.FileWriter(self.options.logDir, sess.graph)\n",
    "log.debug(\"init started\")\n",
    "offset = 0\n",
    "iters = 1  # testing\n",
    "log.info(\"Training for {} iters\".format(iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "loss: 5.018476963043213\n"
     ]
    }
   ],
   "source": [
    "for step in range(iters):\n",
    "    offset = offset % len(tensor_inputs)\n",
    "    if offset <= (len(tensor_inputs) - options.batch_size):\n",
    "        batch_data = tensor_inputs[offset: offset + options.batch_size]\n",
    "        batch_labels = tensor_labels[offset: offset + options.batch_size]\n",
    "        offset += options.batch_size\n",
    "    else:\n",
    "        to_add = options.batch_size - (len(tensor_inputs) - offset)\n",
    "        batch_data = np.concatenate((tensor_inputs[offset: len(tensor_inputs)], tensor_inputs[0: to_add]))\n",
    "        batch_labels = np.concatenate((tensor_labels[offset: len(tensor_inputs)], tensor_labels[0: to_add]))\n",
    "        offset = to_add\n",
    "    print(\"starting\")\n",
    "    # this is finally it ... GEEZEEZEZ\n",
    "    feed_dict={input_holder: batch_data, input_label: batch_labels}\n",
    "    # step = sess.run(global_step)\n",
    "    # print(\"Global Step: {}\".format(step))\n",
    "    # learn = sess.run(learn_rate)\n",
    "    # print(\"learn rate: {}\".format(learn_rate))\n",
    "    # cost = sess.run(cost)\n",
    "    # print(\"cost: {}\".format(cost))\n",
    "    _, loss = sess.run([train_op,cost], feed_dict=feed_dict)\n",
    "    print(\"loss: {:2f}\".format(loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log.info(\"Starting the wikigen Training!\")\n",
    "#with tf.Graph().as_default():\n",
    "sess = tf.Session()\n",
    "log.debug(\"session started\")\n",
    "#init graph, load model\n",
    "sess.run(init_op())\n",
    "train_writer = tf.summary.FileWriter(save_path, sess.graph)\n",
    "# test_writer = tf.summary.FileWriter(self.options.logDir, sess.graph)\n",
    "log.debug(\"init started\")\n",
    "offset = 0\n",
    "iters = 1  # testing\n",
    "log.info(\"Training for {} iters\".format(iters))\n",
    "for step in range(iters):\n",
    "    offset = offset % len(tensor_inputs)\n",
    "    if offset <= (len(tensor_inputs) - options.batch_size):\n",
    "        batch_data = tensor_inputs[offset: offset + options.batch_size]\n",
    "        batch_labels = tensor_labels[offset: offset + options.batch_size]\n",
    "        offset += options.batch_size\n",
    "    else:\n",
    "        to_add = options.batch_size - (len(tensor_inputs) - offset)\n",
    "        batch_data = np.concatenate((tensor_inputs[offset: len(tensor_inputs)], tensor_inputs[0: to_add]))\n",
    "        batch_labels = np.concatenate((tensor_labels[offset: len(tensor_inputs)], tensor_labels[0: to_add]))\n",
    "        to_add = batch_size - (len(X) - offset)\n",
    "        batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "        batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "        offset = to_add\n",
    "        offset = to_add\n",
    "\n",
    "    # this is finally it ... GEEZEEZEZ\n",
    "    feed_dict={input_holder: batch_data, input_label: batch_labels}\n",
    "    _, current_step, summary, cost, learn = sess.run([train_op,\n",
    "                                                      global_step,\n",
    "                                                      merged,\n",
    "                                                      cost,\n",
    "                                                      learn_rate], \n",
    "                                                     feed_dict)\n",
    "\n",
    "    # _, training_loss = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "\n",
    "    if step % log_every == 0:\n",
    "        # print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "        train_writer.add_summary(summary, step)\n",
    "        log.info(\"{} :: Current Step: {}\\n\\t-Loss: {}\\n\\t-Learn rate: {}\\n{}\\n\".format(datetime.time(), current_step, cost, learn, buf))\n",
    "\n",
    "        if step % test_every == 0:\n",
    "            reset_test_state.run()\n",
    "            test_generated = starting_text\n",
    "            log.debug(\"in a testing phase\")\n",
    "            for i in range(len(starting_text) - 1):\n",
    "                test_X = np.zeros((1, char_size))\n",
    "                test_X[0, char2id[starting_text[i]]] = 1.\n",
    "                test_feed={test_data: test_X}\n",
    "                _ = sess.run(test_op, feed_dict=test_feed)\n",
    "            # test generated\n",
    "            test_X = np.zeros((1, char_size))\n",
    "            test_X[0, char2id[starting_text[-1]]] = 1.\n",
    "            # creating test\n",
    "            for i in range(options.max_text):\n",
    "                prediction = test_op.eval({test_data: test_X})[0]\n",
    "                next_char_one_hot = sample(prediction)\n",
    "                next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "                test_generated += next_char\n",
    "                test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "            print(buf)\n",
    "            print(test_generated)\n",
    "            print(buf)\n",
    "\n",
    "            # saver.save(sess, save_path, global_step=step)\n",
    "saver.save(sess, save_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# starting_text\n",
    "#init graph, load model\n",
    "sess.run(init_op())\n",
    "log.info(\"Initilized Session\")\n",
    "# tf.global_variables_initializer().run()\n",
    "model = tf.train.latest_checkpoint(save_path)\n",
    "saver.restore(sess, model)\n",
    "log.info(\"Restored most current training\")\n",
    "# set input variable to generate chars from\n",
    "sess.run(reset_test_state)\n",
    "test_generated = test_start\n",
    "log.info(\"First test set generated.\")\n",
    "#for every char in the input sentennce\n",
    "for i in range(len(starting_text) - 1):\n",
    "    #initialize an empty char store\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    #store it in id from\n",
    "    test_X[0, char2id[starting_text[i]]] = 1.\n",
    "    #feed it to model, test_prediction is the output value\n",
    "    _ = sess.run(test_op, feed_dict={test_data: test_X})\n",
    "log.info(\"Test Run.\")\n",
    "\n",
    "#where we store encoded char predictions\n",
    "test_X = np.zeros((1, char_size))\n",
    "test_X[0, char2id[starting_text[-1]]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#lets generate 500 characters\n",
    "for i in range(500):\n",
    "    #get each prediction probability\n",
    "    prediction = test_op.eval({test_data: test_X})[0]\n",
    "    #one hot encode it\n",
    "    next_char_one_hot = sample(prediction)\n",
    "    #get the indices of the max values (highest probability)  and convert to char\n",
    "    next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "    #add each char to the output text iteratively\n",
    "    test_generated += next_char\n",
    "    #update the \n",
    "    test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "print(test_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eric @ genRuckus 3.5",
   "language": "python",
   "name": "genruckus35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
