{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "working_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ag.logging as log\n",
    "from PIL import Image\n",
    "log.set(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "load = np.load(\"super_set.npz\")\n",
    "imgs = load['images']\n",
    "labels = load['labels']\n",
    "dataset_examples = imgs.shape[0]\n",
    "dataset_h = imgs.shape[1]\n",
    "dataset_w = imgs.shape[2]\n",
    "dataset_c = imgs.shape[3]\n",
    "dataset_classes = labels.shape[1]\n",
    "dataset_shape = imgs[0].shape\n",
    "label_example = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log.info(\"Images: {}\".format(imgs.shape))\n",
    "log.info(\"Labels: {}\".format(labels.shape))\n",
    "log.info(\"Image shape: {}\".format(dataset_shape))\n",
    "log.info(\"Label: {}\".format(label_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"InitVars\"):\n",
    "    with tf.variable_scope(\"init_vars_values\"):\n",
    "        global_step = tf.Variable(name='global_step',\n",
    "                                  trainable=False,\n",
    "                                  initial_value=0,\n",
    "                                  collections=[\"echo\", \"step\"])\n",
    "        \n",
    "        learn_rate = tf.train.exponential_decay(0.1, global_step,\n",
    "                                                .05, 0.87,\n",
    "                                                staircase=True, name=\"Learn_decay\")\n",
    "        \n",
    "tf.add_to_collection(\"learn_rate\", learn_rate)\n",
    "tf.summary.scalar(\"learn_rate\", learn_rate)\n",
    "tf.summary.scalar(\"global_step\", learn_rate)\n",
    "tf.summary.histogram('decay_rate', learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"inputs\"):\n",
    "    Input_Tensor_Image = tf.placeholder(tf.float32, [None, dataset_h, dataset_w,\n",
    "                                                           dataset_c], name=\"Input_Tensor\")\n",
    "tf.add_to_collection(\"input_tensor\", Input_Tensor_Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Input_Tensor_Labels = tf.placeholder(tf.float32, [None, dataset_classes], name=\"Input_Label\")\n",
    "tf.add_to_collection(\"label\", Input_Tensor_Labels)\n",
    "Input_True_Labels = tf.argmax(Input_Tensor_Labels, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"keep_prob\"):\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"Keep_prob\")  # new feature goes with the dropout option\n",
    "    tf.add_to_collection(\"keep_prob\", keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def BUILD_CONV_LAYERS(x_image, layers, f_size=5):\n",
    "    \"\"\"\n",
    "    Magically construct many Convolutional layers for a Neural Network.\n",
    "    :return:\n",
    "        An unused list of all the layers_names, weights and biases for each layer. these are added to tensorboard\n",
    "        automatically.\n",
    "    \"\"\"\n",
    "    start_shape = x_image\n",
    "    log.debug(\"Start shape= {}\".format(start_shape))\n",
    "    reducing_shape = 0\n",
    "    last_num_f = 0\n",
    "    log.info(\"Building {} Convolutional Layers\".format(layers))\n",
    "    for layer in range(layers):\n",
    "        num_f = 16 * (layer + 1)\n",
    "        log.debug(\"Current Num of Features= {}\".format(num_f))\n",
    "        if layer == 0:\n",
    "            img = start_shape\n",
    "            channel = dataset_c\n",
    "        elif layer > 0:\n",
    "            img = reducing_shape\n",
    "            channel = last_num_f\n",
    "\n",
    "        last_num_f = num_f\n",
    "        log.debug(\"Start shape= {}\".format(start_shape))\n",
    "        debugs = \"New Features = {}\".format(channel)\n",
    "        reducing_shape, w, b = new_conv_layer(input=img,\n",
    "                                              filter_size=f_size,\n",
    "                                              chan=channel,\n",
    "                                              num_filters=num_f)\n",
    "        layer_name = \"convLayer_%s\" % layer\n",
    "        log.debug(\"Finishing Layer {}\\n\\t{}\".format(layer_name, debugs))\n",
    "        tf.summary.histogram(\"weights{}\".format(layer), w)\n",
    "        tf.summary.histogram(\"biases{}\".format(layer), b)\n",
    "        log.info(\"Finished Layer {}\\n\\t{}\".format(layer_name, debugs))\n",
    "\n",
    "    log.info(\"Finished Building {} Conv Layers\\nMoving To Flattening...\".format(layers))\n",
    "    return reducing_shape, last_num_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BUILD_FC_LAYER(x_image, in_features, num_fc, num_final):\n",
    "    \"\"\"\n",
    "    Magically create a huge number of Fully connected layers.\n",
    "    :param layers:\n",
    "        Number of FC layers to create.\n",
    "    :return:\n",
    "        returns a human readable non used list of the layer names. These are added to tensorboard automatically.\n",
    "    \"\"\"\n",
    "    log.info(\"Building {} Fully Connected Layers\\ninput features size = {}\\n*-\".format(num_fc, in_features))\n",
    "    # first time thru options\n",
    "    inputs = in_features\n",
    "    features = 0\n",
    "    use_reLu = True\n",
    "    use_Drop = True\n",
    "    current_layer = False    \n",
    "    for layer in range(num_fc):        \n",
    "        if layer == 0:  # if first time through\n",
    "            current_layer = x_image\n",
    "            \n",
    "        else:\n",
    "            inputs = features\n",
    "        \n",
    "        features = int(inputs / (layer + 1)) # avoid a 0 zero divion error..\n",
    "        \n",
    "        if layer == num_fc - 1:  # if last time through\n",
    "            features = num_final\n",
    "            use_reLu = False  # dont use on the last time thru\n",
    "            use_Drop = False\n",
    "        \n",
    "        log.debug(\"Layer-{}\\n\\t# inputs: {}\\n\\t# outputs: {}\\n\\tuse relu? {}\\n\\tuse drop? {}\".format(\\\n",
    "            current_layer, inputs, features, use_reLu, use_Drop))\n",
    "        current_layer, w, b = new_fc_layer(input_=current_layer,\n",
    "                                           num_inputs=inputs,\n",
    "                                           num_outputs=features,\n",
    "                                           use_relu=use_reLu,\n",
    "                                           use_drop=use_Drop)\n",
    "             \n",
    "        layer_name = \"fullyLayer_{}\".format(layer)\n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope(\"weights\"):\n",
    "                tf.summary.histogram(\"weights\", w)\n",
    "            with tf.name_scope(\"biases\"):\n",
    "                tf.summary.histogram('biases', b)\n",
    "        log.info(\"{} : {}\".format(layer_name, current_layer))\n",
    "    log.info(\"Finished Building {} Fully Connected Layers\".format(num_fc))\n",
    "    return current_layer # final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input_, num_inputs, num_outputs, use_relu=True, use_drop=False):\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])  # set weights\n",
    "    biases = new_biases(length=num_outputs)  # set number of OUTPUTS like give me top k or whatever...\n",
    "    layer = tf.matmul(input_, weights) + biases  # this is a #BIGMATH func\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    if use_drop:\n",
    "        layer = tf.nn.dropout(layer, keep_prob)\n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    \"\"\"This generates a new weight for each layer\"\"\"\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"weight\", collections=[\"weights\"])\n",
    "\n",
    "def new_biases(length):\n",
    "    \"\"\"This generates a new bias for each layer\"\"\"\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=\"bias\", collections=[\"biases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input, filter_size, chan, num_filters, use_pooling=True):\n",
    "    log.debug(\"Building a conv layer\")\n",
    "    X_shape = [filter_size, filter_size, chan, num_filters]\n",
    "    log.debug(\"current shape: {}\".format(X_shape))\n",
    "    weights = new_weights(shape=X_shape)\n",
    "    log.debug(\"weights shape = {}\".format(weights))\n",
    "    biases = new_biases(length=num_filters)\n",
    "    log.debug(\"weights shape = {}\".format(biases))\n",
    "    \"\"\" THis is the MAGIC again... \"\"\"\n",
    "    layer = tf.nn.conv2d(input=input,  # This is the output of the last layer\n",
    "                         filter=weights,  # this is a thing\n",
    "                         strides=[1, 1, 1, 1],  # 1111 is NOT pooled MAX work\n",
    "                         padding='SAME')  # input output transformation\n",
    "    layer += biases  # Add biases\n",
    "    if use_pooling:  # this skips pixels... saves time but skips things obviously\n",
    "        log.debug(\"using pooling\")\n",
    "        layer = tf.nn.max_pool(value=layer,  # take the weights and bias together as an input\n",
    "                               ksize=[1, 2, 2, 1],  # stuff\n",
    "                               strides=[1, 2, 2, 1],  # 2 x 2 stride... could increase... check\n",
    "                               padding='SAME')  # i feel like this should already be in variable, but w/e\n",
    "    layer = tf.nn.relu(layer)  # rectified linear Unit ... like a boss\n",
    "    log.debug(\"Finished Building a conv Layer:\\n\\t{}\".format(layer))\n",
    "    return layer, weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    log.info(\"Tranitioning from Conv to fc layers\")\n",
    "    layer_shape = layer.get_shape()  # ASSERT layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "    num_features = layer_shape[1:4].num_elements()  # like a boss...\n",
    "    log.debug(\"Shape: {}, Features: {}\".format(layer_shape, num_features))\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])  # yep...\n",
    "    log.info(\"Finished Flattening Layers\")\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv_layers\"):\n",
    "    x_image, num_features = BUILD_CONV_LAYERS(Input_Tensor_Image, 2)\n",
    "log.info(\"created {} layers with {} features\".format(2, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Flat_layer\"):\n",
    "    x_image, features =flatten_layer(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"FC_layers\"):\n",
    "    final_layer = BUILD_FC_LAYER(x_image, features, num_fc=5, num_final=5)  # build FC LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"sophmax\"):\n",
    "    sophmax = tf.nn.softmax(final_layer, name=\"sophmax\")\n",
    "    tf.add_to_collection(\"sophmax_layer\", sophmax)\n",
    "tf.summary.histogram('activations', sophmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### TRAINING METHOD FROM MUPEN #### MEPHOD?\n",
    "with tf.variable_scope(\"mupen_method\"):\n",
    "    cost = tf.reduce_mean(tf.square(tf.subtract(Input_Tensor_Labels, final_layer)))\n",
    "    training_vars = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()]) * 0.001\n",
    "    loss = cost + training_vars\n",
    "    tf.add_to_collection(\"loss\", loss)\n",
    "tf.summary.scalar(\"train_cost\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Dropout_Optimizer_Train'):\n",
    "     train_drop_loss = tf.train.AdamOptimizer(learn_rate).minimize(loss, global_step=global_step)\n",
    "     tf.add_to_collection(\"train_op\", train_drop_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### OHHHH WE ARE GETTING CLOSE! ###\n",
    "\"\"\" Initialize the session \"\"\"\n",
    "init_op = tf.global_variables_initializer\n",
    "\n",
    "\"\"\"create summary op\"\"\"\n",
    "merged = tf.summary.merge_all\n",
    "\n",
    "\"\"\"create saver object\"\"\"\n",
    "saver = tf.train.Saver(\n",
    "            #var_list={\"{}\".format(v): v for v in [tf.model_variables()]},\n",
    "            write_version=tf.train.SaverDef.V2,\n",
    "            sharded=True,\n",
    "            keep_checkpoint_every_n_hours=.001\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Session Manger for distrubted train... SOON!!!###\n",
    "#self.bossMan = tf.train.Supervisor(is_chief=True,\n",
    "#                                   logdir='~/notebooks/logdir/',\n",
    "#                                   checkpoint_basename='check.point',\n",
    "#                                   init_op = init_op,\n",
    "#                                   summary_op = merged,\n",
    "#                                   saver = saver,\n",
    "#                                   global_step = global_step,\n",
    "#                                   save_model_secs = 60\n",
    "#                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### last thing we need is a batch mechiniism...\n",
    "def get_batch(): pass                                                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Single use for demonstration Session...\n",
    "# Start session\n",
    "sess = tf.InteractiveSession()\n",
    "log.info(\"Started Session: {}\".format(sess))\n",
    "sess.run(init_op())\n",
    "log.info(\"started Init Op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training loop variables\n",
    "epochs = 100\n",
    "batch_size = 50\n",
    "num_samples = imgs.shape[0]\n",
    "step_size = int(num_samples / batch_size) \n",
    "log.info(\"Training Run Options:\\n\\tTraining data resets: {}\\n\\tBatch size: {}\\n\\tStep Size: {}\\n\\t\".format(\\\n",
    "            epochs, batch_size, step_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Training Proceedures\n",
    "current_epoch = 0\n",
    "\n",
    "while current_epoch < epochs:\n",
    "    log.info(\"Currently on epoch: {}\".format(current_epoch))\n",
    "    for i in range(step_size):\n",
    "        feed_dict = {Input_Tensor_Images: batch[0],\n",
    "                     Input_Tensor_Labels: batch[1],\n",
    "                     keep_prob: 0.8}\n",
    "\n",
    "        _, current_step, summary, loss, learn = sess.run([train_drop_loss,\n",
    "                                                          merged,\n",
    "                                                          learn_rate], \n",
    "                                                         feed_dict)\n",
    "\n",
    "    log.info(\"\\t-Current Step: {}\\n\\t-Loss: {}\\n\\t-Learn rate: {}\".format(current_step, loss, learn))\n",
    "\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log.info(\"Starting the Ipython Training!\")\n",
    "sess = tf.Session()\n",
    "log.debug(\"session started\")\n",
    "#init graph, load model\n",
    "sess.run(init_op())\n",
    "train_writer = tf.summary.FileWriter(save_path, sess.graph)\n",
    "# test_writer = tf.summary.FileWriter(self.options.logDir, sess.graph)\n",
    "log.debug(\"init started\")\n",
    "offset = 0\n",
    "iters = 1  # testing\n",
    "log.info(\"Training for {} iters\".format(iters))\n",
    "for step in range(iters):\n",
    "    feed_dict={input_holder: batch_data, input_label: batch_labels}\n",
    "    _, current_step, summary, cost, learn = sess.run([train_op,\n",
    "                                                      global_step,\n",
    "                                                      merged,\n",
    "                                                      cost,\n",
    "                                                      learn_rate], \n",
    "                                                     feed_dict)\n",
    "\n",
    "    # _, training_loss = sess.run([train_op, cost], feed_dict=feed_dict)\n",
    "\n",
    "    if step % log_every == 0:\n",
    "        # print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "        train_writer.add_summary(summary, step)\n",
    "        log.info(\"{} :: Current Step: {}\\n\\t-Loss: {}\\n\\t-Learn rate: {}\\n{}\\n\".format(datetime.time(), current_step, cost, learn, buf))\n",
    "\n",
    "        if step % test_every == 0:\n",
    "            # creating test\n",
    "            prediction = test_op.eval({test_data: test_X})\n",
    "\n",
    "            print(buf)\n",
    "            print(\"test_loss: {}\".format(prediction))\n",
    "            print(buf)\n",
    "\n",
    "            # saver.save(sess, save_path, global_step=step)\n",
    "saver.save(sess, save_path, global_step=step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agPy3.6",
   "language": "python",
   "name": "agpy3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
